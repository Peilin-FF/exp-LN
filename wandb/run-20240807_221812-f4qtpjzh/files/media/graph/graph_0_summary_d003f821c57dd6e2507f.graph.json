{"format": "torch", "nodes": [{"name": "dec_embedding", "id": 140710871836624, "class_name": "DataEmbedding_wo_pos(\n  (value_embedding): TokenEmbedding(\n    (tokenConv): Conv1d(321, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n  )\n  (position_embedding): PositionalEmbedding()\n  (temporal_embedding): TimeFeatureEmbedding(\n    (embed): Linear(in_features=4, out_features=32, bias=False)\n  )\n  (dropout): Dropout(p=0.05, inplace=False)\n)", "parameters": [["value_embedding.tokenConv.weight", [32, 321, 3]], ["temporal_embedding.embed.weight", [32, 4]]], "output_shape": [[32, 288, 32]], "num_parameters": [30816, 128]}, {"name": "mamba_layers.0", "id": 140710872576720, "class_name": "Mamba2_Layer(\n  (mamba2): Mamba2_LN(\n    (in_proj): Linear(in_features=32, out_features=1096, bias=False)\n    (conv1d): Conv1d(576, 576, kernel_size=(2,), stride=(1,), padding=(1,), groups=576)\n    (act): SiLU()\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (out_proj): Linear(in_features=512, out_features=32, bias=False)\n  )\n)", "parameters": [["mamba2.dt_bias", [8]], ["mamba2.A_log", [8]], ["mamba2.D", [8]], ["mamba2.in_proj.weight", [1096, 32]], ["mamba2.conv1d.weight", [576, 1, 2]], ["mamba2.conv1d.bias", [576]], ["mamba2.norm.weight", [512]], ["mamba2.norm.bias", [512]], ["mamba2.out_proj.weight", [32, 512]]], "output_shape": [[32, 288, 32]], "num_parameters": [8, 8, 8, 35072, 1152, 576, 512, 512, 16384]}, {"name": "mamba_layers.1", "id": 140710855630160, "class_name": "Mamba2_Layer(\n  (mamba2): Mamba2_LN(\n    (in_proj): Linear(in_features=32, out_features=1096, bias=False)\n    (conv1d): Conv1d(576, 576, kernel_size=(2,), stride=(1,), padding=(1,), groups=576)\n    (act): SiLU()\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (out_proj): Linear(in_features=512, out_features=32, bias=False)\n  )\n)", "parameters": [["mamba2.dt_bias", [8]], ["mamba2.A_log", [8]], ["mamba2.D", [8]], ["mamba2.in_proj.weight", [1096, 32]], ["mamba2.conv1d.weight", [576, 1, 2]], ["mamba2.conv1d.bias", [576]], ["mamba2.norm.weight", [512]], ["mamba2.norm.bias", [512]], ["mamba2.out_proj.weight", [32, 512]]], "output_shape": [[32, 288, 32]], "num_parameters": [8, 8, 8, 35072, 1152, 576, 512, 512, 16384]}, {"name": "norm", "id": 140710855629392, "class_name": "LayerNorm((32,), eps=1e-05, elementwise_affine=True)", "parameters": [["weight", [32]], ["bias", [32]]], "output_shape": [[32, 288, 32]], "num_parameters": [32, 32]}, {"name": "out_proj", "id": 140710855628816, "class_name": "Linear(in_features=32, out_features=321, bias=True)", "parameters": [["weight", [321, 32]], ["bias", [321]]], "output_shape": [[32, 288, 321]], "num_parameters": [10272, 321]}], "edges": []}